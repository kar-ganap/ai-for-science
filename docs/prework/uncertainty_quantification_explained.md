# Uncertainty Quantification in Economic Active Learning

**Purpose:** Document our approach to uncertainty estimation and its limitations

**Last Updated:** October 8, 2025

---

## Two Types of Uncertainty

### 1. Epistemic Uncertainty (Knowledge Uncertainty)
**Definition:** Uncertainty due to lack of knowledge/data

**Characteristics:**
- ‚úÖ **Reducible:** Can be decreased by collecting more data
- üìä **Source:** Model has insufficient training examples in this region
- üéØ **Active Learning Target:** This is what we want to identify!

**Example:**
```
Scenario: We've trained on 100 Zn-MOFs but only 5 Zr-MOFs

Prediction on new Zn-MOF:
  - Low epistemic uncertainty (we have lots of similar examples)

Prediction on new Zr-MOF:
  - High epistemic uncertainty (sparse training data in this region)
```

**Visual Intuition:**
```
Data density:
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà Zn-MOFs (100 samples) ‚Üí Low epistemic uncertainty
‚ñà‚ñà Zr-MOFs (5 samples) ‚Üí High epistemic uncertainty
```

---

### 2. Aleatoric Uncertainty (Data Uncertainty)
**Definition:** Uncertainty due to inherent randomness/noise in the system

**Characteristics:**
- ‚ùå **Irreducible:** Cannot be decreased by collecting more data
- üìä **Source:** Measurement noise, stochastic processes, inherent variability
- üéØ **Active Learning:** Not helpful - uncertainty doesn't decrease with more samples

**Example:**
```
CO2 uptake measurements have ¬±0.5 mmol/g experimental error

Even with 1000 measurements of the SAME MOF:
  - Measurements will vary by ¬±0.5 mmol/g
  - This is aleatoric uncertainty (irreducible)
```

**Visual Intuition:**
```
Repeated measurements of MOF-5:
Trial 1: 10.2 mmol/g
Trial 2: 9.8 mmol/g
Trial 3: 10.5 mmol/g
Trial 4: 9.9 mmol/g
Trial 5: 10.1 mmol/g

True value unknown due to measurement noise (aleatoric)
```

---

## Our Approach: Ensemble-Based Uncertainty

### Implementation

```python
# Train ensemble of Random Forests
models = []
for i in range(5):
    model = RandomForestRegressor(
        n_estimators=100,
        random_state=i,    # Different initialization
        max_depth=10
    )
    model.fit(X_train, y_train)
    models.append(model)

# Predict with uncertainty
predictions = [model.predict(X_test) for model in models]
mean = np.mean(predictions, axis=0)
std = np.std(predictions, axis=0)  # ‚Üê Our uncertainty estimate
```

### Sources of Variance in Our Ensemble

#### 1. Random Forest Internal Randomness
**Each individual RF has variance from:**
- Bootstrap sampling (each tree sees ~63% of data)
- Random feature selection at each split
- Random tie-breaking

**Effect:**
- Even a SINGLE RF gives slightly different predictions on repeated training
- This is by design (bias-variance tradeoff)

#### 2. Ensemble Variance (What We Add)
**Across different RF models:**
- Different random seeds ‚Üí different bootstrap samples
- Different tree structures learned
- Different feature subsets chosen

**Effect:**
- Models disagree more when data is sparse or ambiguous
- Agreement indicates robust patterns in data

---

## What Our Ensemble Captures (Honest Assessment)

### ‚úÖ What We Capture Well

#### 1. Data Sparsity Uncertainty (Core Epistemic)
```python
# Example: Sparse vs. Dense regions

# Dense region (100 training samples nearby)
Model 1: 10.1 mmol/g
Model 2: 10.0 mmol/g
Model 3: 10.2 mmol/g
Model 4: 9.9 mmol/g
Model 5: 10.1 mmol/g
‚Üí Std: 0.11 (LOW uncertainty) ‚úÖ

# Sparse region (5 training samples nearby)
Model 1: 8.2 mmol/g
Model 2: 12.5 mmol/g
Model 3: 7.8 mmol/g
Model 4: 15.1 mmol/g
Model 5: 9.2 mmol/g
‚Üí Std: 3.10 (HIGH uncertainty) ‚úÖ
```

**Why this works:**
- When training data is sparse, different bootstrap samples lead to very different trees
- Models learn different patterns from limited data
- High disagreement ‚Üí High uncertainty

#### 2. Model Structure Uncertainty
```python
# Different decision boundaries learned

Model 1 learns: "If ASA > 2000, then high CO2"
Model 2 learns: "If LCD > 15 AND PLD > 10, then high CO2"
Model 3 learns: "Complex interaction between all features"

If data doesn't strongly constrain the model ‚Üí predictions vary
```

#### 3. Feature Interaction Uncertainty
- When feature interactions are complex but data is limited
- Different models may capture different aspects
- Variance indicates "we're not sure which pattern is real"

---

### ‚ùå What We Don't Capture

#### 1. Aleatoric Uncertainty (Inherent Noise)
```python
# Problem: All models see the SAME noisy training data

Training data (with noise):
  MOF-5: 10.2 mmol/g (true: 10.0, noise: +0.2)
  MOF-5: 9.8 mmol/g (true: 10.0, noise: -0.2)
  ...

All 5 models train on this noisy data
‚Üí Each model learns to "average out" the noise
‚Üí Ensemble variance doesn't capture this noise
```

**Solution (not implemented):**
- Heteroscedastic models that predict œÉ¬≤(x) in addition to Œº(x)
- Requires special loss function and training

#### 2. Model Class Uncertainty
```python
# Problem: All models are Random Forests

What if RF is fundamentally wrong for this problem?
  - Neural networks might give different predictions
  - Gaussian Processes might have different uncertainty
  - Linear models might suggest different patterns

We don't capture uncertainty about our model CHOICE
```

**Solution (not implemented):**
- Ensemble of DIFFERENT model types (RF + GBM + NN)
- Much more expensive computationally

#### 3. Feature Uncertainty
```python
# Problem: We assume features are correct

What if LCD, PLD, ASA don't fully capture CO2 uptake?
  - Maybe we need chemical composition features
  - Maybe we need electronic structure descriptors
  - Maybe we're missing critical physics

Our ensemble can't tell us this
```

**Solution (not implemented):**
- Feature importance analysis
- Physics-informed features
- Domain expert validation

---

## Why This Is Still Okay for Active Learning

### Active Learning Cares About Epistemic Uncertainty

**The Goal:**
```
Find regions where model is uncertain DUE TO LACK OF DATA
‚Üí Validate samples from those regions
‚Üí Uncertainty decreases
‚Üí Model improves
```

**What Happens With Different Uncertainty Types:**

| Uncertainty Type | Validate Sample | Result |
|------------------|-----------------|--------|
| **Epistemic (high)** | Yes | ‚úÖ Uncertainty decreases, model improves |
| **Epistemic (low)** | No | ‚ö†Ô∏è Wasted validation, learn little |
| **Aleatoric (high)** | Yes/No | ‚ùå Uncertainty stays high, limited learning |

**Our ensemble targets epistemic uncertainty, which is exactly what we want!**

---

## Empirical Validation of Our Approach

### Test 1: Uncertainty Should Decrease Over AL Iterations

```python
# Expectation:
Iteration 0: Mean uncertainty = 5.2 mmol/g (epistemic high, sparse data)
Iteration 1: Mean uncertainty = 4.1 mmol/g (validated high-uncertainty regions)
Iteration 2: Mean uncertainty = 3.3 mmol/g (continues to decrease)
Iteration 3: Mean uncertainty = 2.8 mmol/g (approaching aleatoric floor)
```

**If epistemic uncertainty dominates:**
- ‚úÖ Uncertainty decreases as we add data
- ‚úÖ Validates our approach

**If aleatoric dominates:**
- ‚ùå Uncertainty stays constant
- ‚ùå Our approach is just measuring noise

### Test 2: Uncertainty Should Correlate with Error

```python
# On held-out test set:

High uncertainty samples (std > 4.0):
  - Mean absolute error: 3.2 mmol/g ‚Üê Predictions less reliable

Low uncertainty samples (std < 1.0):
  - Mean absolute error: 0.8 mmol/g ‚Üê Predictions more reliable

Correlation(uncertainty, error) > 0.7 ‚Üí Good calibration ‚úÖ
```

### Test 3: Economic AL Should Outperform Random

```python
# Compare to random baseline:

Economic AL (our approach):
  - After 500 validations: RMSE = 1.2 mmol/g

Random sampling baseline:
  - After 500 validations: RMSE = 2.1 mmol/g

Economic AL is 2√ó more efficient ‚Üí Uncertainty is meaningful ‚úÖ
```

---

## More Rigorous Approaches (For Reference)

### 1. Bayesian Neural Networks

**Method:**
```python
# Place prior distributions over weights
p(w) = N(0, œÉ_prior¬≤)

# Posterior predictive distribution
p(y|x, D) = ‚à´ p(y|x, w) p(w|D) dw

# Captures epistemic + aleatoric uncertainty
```

**Pros:**
- ‚úÖ Principled Bayesian framework
- ‚úÖ Naturally separates epistemic/aleatoric
- ‚úÖ Well-calibrated uncertainties

**Cons:**
- ‚ùå Expensive to train (MCMC or variational inference)
- ‚ùå Requires careful prior selection
- ‚ùå Harder to implement

### 2. Gaussian Processes

**Method:**
```python
from sklearn.gaussian_process import GaussianProcessRegressor

gp = GaussianProcessRegressor(kernel=RBF())
gp.fit(X_train, y_train)

# Naturally provides uncertainty
y_mean, y_std = gp.predict(X_test, return_std=True)
```

**Pros:**
- ‚úÖ Gold standard for uncertainty quantification
- ‚úÖ Exact posterior (not approximation)
- ‚úÖ Theoretically grounded

**Cons:**
- ‚ùå O(n¬≥) complexity - doesn't scale
- ‚ùå Struggles with high dimensions
- ‚ùå Not practical for 10,000+ MOFs

### 3. Deep Ensembles

**Method:**
```python
# 5-10 neural networks with different initializations
models = [
    NeuralNet(layers=[64, 32, 16], init=i)
    for i in range(10)
]

# Same idea as our RF ensemble but with NNs
```

**Pros:**
- ‚úÖ State-of-the-art epistemic uncertainty
- ‚úÖ Scales to large datasets
- ‚úÖ Better than single NN with dropout

**Cons:**
- ‚ùå More expensive than RF ensemble
- ‚ùå Requires hyperparameter tuning
- ‚ùå Still doesn't capture aleatoric directly

### 4. Heteroscedastic Models

**Method:**
```python
# Model predicts BOTH mean and variance
def loss(y_true, mu_pred, sigma_pred):
    # Negative log-likelihood
    return 0.5 * log(sigma_pred¬≤) + (y_true - mu_pred)¬≤ / (2 * sigma_pred¬≤)

# sigma_pred captures aleatoric uncertainty
# Ensemble variance captures epistemic uncertainty
```

**Pros:**
- ‚úÖ Explicitly models aleatoric uncertainty
- ‚úÖ Can be combined with ensembles
- ‚úÖ More complete uncertainty quantification

**Cons:**
- ‚ùå Requires custom loss function
- ‚ùå More complex training
- ‚ùå May need more data to estimate variance

---

## Our Justification for Hackathon

### Why Our Approach Is Sufficient

**1. Primary Goal is Relative Ranking**
```
We don't need EXACT uncertainty values
We need to RANK samples: "Which is most uncertain?"

Our ensemble does this well even if absolute values are imperfect
```

**2. Epistemic > Aleatoric for AL**
```
Active Learning targets epistemic uncertainty
Aleatoric uncertainty doesn't decrease with data
So focusing on epistemic is the right choice
```

**3. Computational Efficiency**
```
RF ensemble:
  - 5 models √ó 1 minute = 5 minutes training
  - Fast enough for interactive demo

Bayesian NN or GP:
  - 30+ minutes training
  - Too slow for hackathon demo
```

**4. Proven in Literature**
```
Ensemble-based uncertainty is standard in:
  - Drug discovery active learning
  - Materials informatics
  - AutoML model selection

It works in practice, even if not theoretically perfect
```

---

## Implementation Notes

### Current Implementation

```python
class EconomicActiveLearner:
    def train_ensemble(self, n_models=5):
        """
        Train ensemble for uncertainty quantification

        Captures:
          ‚úÖ Epistemic uncertainty (data sparsity)
          ‚úÖ Model structure uncertainty
          ‚ùå Aleatoric uncertainty (inherent noise)

        Sufficient for:
          ‚úÖ Active Learning sample selection
          ‚úÖ Relative uncertainty ranking
          ‚úÖ Cost-aware prioritization
        """
        self.models = []
        for i in range(n_models):
            model = RandomForestRegressor(
                n_estimators=100,
                random_state=i,  # Different seed per model
                max_depth=10,
                min_samples_split=5
            )
            model.fit(self.X_train, self.y_train)
            self.models.append(model)

    def predict_with_uncertainty(self, X):
        """
        Returns:
            mean: Best estimate of prediction
            std: Uncertainty estimate (primarily epistemic)
        """
        predictions = np.array([m.predict(X) for m in self.models])
        return predictions.mean(axis=0), predictions.std(axis=0)
```

### Potential Future Improvements

```python
# 1. Add aleatoric uncertainty (if needed)
class HeteroscedasticEnsemble:
    def predict_with_full_uncertainty(self, X):
        # Each model predicts mean + variance
        means, vars = zip(*[m.predict_with_variance(X) for m in self.models])

        epistemic = np.std(means, axis=0)  # Disagreement between models
        aleatoric = np.mean(vars, axis=0)  # Average predicted noise
        total = np.sqrt(epistemic**2 + aleatoric**2)

        return mean, epistemic, aleatoric, total

# 2. Multi-model ensemble (if time permits)
class MultiModelEnsemble:
    def __init__(self):
        self.models = [
            RandomForestEnsemble(n=3),
            GradientBoostingEnsemble(n=3),
            NeuralNetEnsemble(n=2)
        ]
    # More robust to model class uncertainty
```

---

## Key Takeaways

### What We Know
1. ‚úÖ Our ensemble captures **epistemic uncertainty reasonably well**
2. ‚úÖ This is **sufficient and appropriate for Active Learning**
3. ‚úÖ Approach is **computationally efficient** for hackathon
4. ‚ùå We're **not capturing aleatoric uncertainty** (but don't need it)
5. ‚ùå We're **not capturing full model uncertainty** (but ensemble helps)

### What This Means Practically
1. ‚úÖ High uncertainty ‚Üí "Model uncertain due to sparse data" ‚Üí Validate
2. ‚úÖ Low uncertainty ‚Üí "Model confident, has seen similar samples" ‚Üí Skip
3. ‚úÖ Uncertainty will **decrease over AL iterations** (empirical validation)
4. ‚ö†Ô∏è Absolute uncertainty values are **approximate** (use for ranking, not inference)

### When to Upgrade
- ‚úÖ **For hackathon:** Current approach is optimal
- ‚ö†Ô∏è **For research paper:** Consider Bayesian methods or GPs
- ‚ö†Ô∏è **For production:** May need calibration and aleatoric modeling
- ‚ö†Ô∏è **For safety-critical:** Definitely need more rigorous UQ

---

## Further Reading

### Ensemble Uncertainty
- Lakshminarayanan et al. (2017) - "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"
- Uncertainty in Deep Learning PhD thesis - Yarin Gal

### Active Learning with Uncertainty
- Settles (2009) - "Active Learning Literature Survey"
- Cohn et al. (1996) - "Active Learning with Statistical Models"

### Materials Informatics Applications
- Tran et al. (2020) - "Active learning across intermetallics to guide discovery of electrocatalysts"
- Lookman et al. (2019) - "Active learning in materials science"

---

**Status:** This document reflects our current understanding and implementation choices.

**Last Updated:** October 8, 2025 by Kartik Ganapathi (with Claude Code)
